In the previous section we describe how to build a model, but we steel do not describe how model works. 
Let's fix this shortcoming. Our model support a multilingual documents, thus document may contain a few text with
different attributes, all texts in one document inherit the same distribution of topics. Thus we have had one matrix $\Phi$ per
attribute and one matrix $\Theta$ for all attribute. In order to train our model we have to
% TODO нарисовать пиздатую картинку из которой все сразу всё поймут.
\begin{enumerate}
    \item Generate some initial approximation for matrix $\Theta$ and every matrices $\Phi$
    \item \label{AlgorithmBegin} Perform E\--step and estimate the number of words $w$ in document $d$, produced by topic $t$ for every attribute.
    \item Apply reqularizer to every matrix. (see \ref{Regularizers})
    \item Perform M\--step for every matrix.
    \item Sparsify matrices $\Phi$ and matrix $\Theta$. (see \ref{sparseModel})
    \item Check the stopping criteria and return training model if it time to stop or return to the step \ref{AlgorithmBegin} otherwise.   
\end{enumerate}

The main part of our project is PLSABricks, it do the main part of work. One brick process one attribute. It performs E\--step, apples
reqularizer to matrix $\Phi$, performs M\--step to the matrix $\Phi$ and sparsifies matrix $\Phi$. PLSA include one brick per attribute,
regularizer and sparsifier for matrix $\Theta$, stopping criteria.  
